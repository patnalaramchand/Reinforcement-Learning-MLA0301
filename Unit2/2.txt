import numpy as np
from collections import defaultdict

# 1. Environment Setup
grid_size = (5, 5)
start_pos = (0, 0)
delivery_points = [(4, 4), (0, 4)]
obstacles = [(2, 2), (2, 3)]
actions = ['up', 'down', 'left', 'right'] # 0, 1, 2, 3

# Rewards
delivery_reward = 50
obstacle_penalty = -30
step_penalty = -1 # Fuel consumption

# 2. Monte Carlo Control Implementation
q_table = defaultdict(lambda: np.zeros(len(actions)))
returns = defaultdict(list)
gamma = 0.95 # Discount factor
epsilon = 0.1 # For epsilon-soft policy
episodes = 5000

def get_policy(state):
    """Epsilon-soft policy."""
    if np.random.rand() < epsilon:
        return np.random.randint(len(actions))
    else:
        return np.argmax(q_table[state])

# 3. Main Loop
for episode in range(episodes):
    # Generate an episode
    episode_history = []
    state = start_pos
    
    while state not in delivery_points:
        action = get_policy(state)
        
        next_state = list(state)
        if action == 0 and state[0] > 0: next_state[0] -= 1
        elif action == 1 and state[0] < grid_size[0] - 1: next_state[0] += 1
        elif action == 2 and state[1] > 0: next_state[1] -= 1
        elif action == 3 and state[1] < grid_size[1] - 1: next_state[1] += 1
        next_state = tuple(next_state)

        reward = step_penalty
        if next_state in obstacles:
            reward = obstacle_penalty
        elif next_state in delivery_points:
            reward = delivery_reward
        
        episode_history.append((state, action, reward))
        state = next_state
        
        # Break if episode is too long
        if len(episode_history) > 50: break

    # Update Q-table using first-visit method
    G = 0
    visited_state_actions = set()
    for state, action, reward in reversed(episode_history):
        G = gamma * G + reward
        if (state, action) not in visited_state_actions:
            returns[(state, action)].append(G)
            q_table[state][action] = np.mean(returns[(state, action)])
            visited_state_actions.add((state, action))

print("Training finished.\n")

# 4. Display Optimal Policy
policy_symbols = ['↑', '↓', '←', '→']
optimal_policy_grid = np.full(grid_size, ' ', dtype=str)

for r in range(grid_size[0]):
    for c in range(grid_size[1]):
        state = (r, c)
        if state in obstacles: optimal_policy_grid[state] = 'X'
        elif state in delivery_points: optimal_policy_grid[state] = 'D'
        else:
            best_action = np.argmax(q_table[state])
            optimal_policy_grid[state] = policy_symbols[best_action]

print("Optimal Drone Policy (D: Delivery, X: Obstacle):")
print(optimal_policy_grid)