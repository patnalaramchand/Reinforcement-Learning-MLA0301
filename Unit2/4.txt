import numpy as np

# 1. Environment Setup
house_layout = np.array([
    [0, 0, 0, -1, 0], # 0: cleanable, -1: obstacle/wall
    [0, -1, 0, 0, 0],
    [0, 0, 0, -1, 0],
    [-1, 0, 0, 0, 0],
    [0, 0, -1, 0, 0]
])
start_pos = (0, 0)
actions = ['up', 'down', 'left', 'right']

# Rewards
cleaned_reward = 5
obstacle_penalty = -5
revisit_penalty = -1 # Penalty for visiting an already clean tile
step_penalty = -0.5 # Energy usage

# 2. SARSA Algorithm
q_table = np.zeros(house_layout.shape + (len(actions),))
alpha = 0.1
gamma = 0.9
epsilon = 0.1
episodes = 3000

def get_action(state, q_values):
    """Epsilon-greedy action selection."""
    if np.random.uniform(0, 1) < epsilon:
        return np.random.choice(4)
    else:
        return np.argmax(q_values[state])

# 3. Main Loop
for episode in range(episodes):
    state = start_pos
    cleaned_tiles = np.zeros_like(house_layout)
    action = get_action(state, q_table)
    
    for step in range(100): # Max steps per episode
        next_state = list(state)
        if action == 0 and state[0] > 0: next_state[0] -= 1
        elif action == 1 and state[0] < house_layout.shape[0] - 1: next_state[0] += 1
        elif action == 2 and state[1] > 0: next_state[1] -= 1
        elif action == 3 and state[1] < house_layout.shape[1] - 1: next_state[1] += 1
        next_state = tuple(next_state)

        reward = step_penalty
        if house_layout[next_state] == -1: # Hit an obstacle
            reward = obstacle_penalty
            next_state = state # Bounce back
        elif cleaned_tiles[next_state] == 0: # Cleaned a new tile
            reward = cleaned_reward
            cleaned_tiles[next_state] = 1
        else: # Re-visited a clean tile
            reward = revisit_penalty

        # Choose next action A' from next state S'
        next_action = get_action(next_state, q_table)
        
        # SARSA Update
        old_q = q_table[state][action]
        next_q = q_table[next_state][next_action]
        q_table[state][action] = old_q + alpha * (reward + gamma * next_q - old_q)
        
        state = next_state
        action = next_action
        
        # End episode if all possible tiles are cleaned
        if np.sum(cleaned_tiles) == np.sum(house_layout == 0):
            break

print("Training finished.\n")

# 4. Display Optimal Policy
policy_symbols = ['↑', '↓', '←', '→']
optimal_policy_grid = np.full(house_layout.shape, ' ', dtype=str)

for r in range(house_layout.shape[0]):
    for c in range(house_layout.shape[1]):
        if house_layout[r, c] == -1: optimal_policy_grid[r, c] = 'X'
        else:
            best_action = np.argmax(q_table[r, c])
            optimal_policy_grid[r, c] = policy_symbols[best_action]

print("Optimal Cleaning Policy (X: Obstacle):")
print(optimal_policy_grid)