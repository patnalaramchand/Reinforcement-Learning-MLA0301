import numpy as np

# 1. Environment Definition
grid_size = (4, 5) # 4 rows, 5 columns
value_function = np.zeros(grid_size)
obstacles = [(1, 1), (1, 2), (2, 2)]
item_location = (0, 4)
goal_location = (3, 4)
actions = ['up', 'down', 'left', 'right']

# Rewards
item_reward = 2
goal_reward = 5
obstacle_penalty = -2
step_penalty = -0.1

# 2. Define a Fixed Policy (Ï€)
# 0: up, 1: down, 2: left, 3: right
# Let's define a simple policy: always try to move right, if not possible, go down.
policy = np.full(grid_size, 3, dtype=int) # Default to 'right'
policy[3, :] = 2 # In last row, try to go left if right is blocked
policy[:, 4] = 1 # In last column, go down

print("Defined Policy (0:up, 1:down, 2:left, 3:right):")
print(policy)

# 3. Policy Evaluation Algorithm
gamma = 0.9 # Discount factor
theta = 1e-6 # Convergence threshold

while True:
    delta = 0
    # Create a copy to store updated values for this iteration
    new_value_function = np.copy(value_function)
    
    for r in range(grid_size[0]):
        for c in range(grid_size[1]):
            state = (r, c)
            
            # If it's a terminal state, value is 0
            if state == goal_location or state == item_location:
                continue

            action = policy[state]
            
            # Determine next state
            next_state = list(state)
            if action == 0 and r > 0: next_state[0] -= 1
            elif action == 1 and r < grid_size[0] - 1: next_state[0] += 1
            elif action == 2 and c > 0: next_state[1] -= 1
            elif action == 3 and c < grid_size[1] - 1: next_state[1] += 1
            next_state = tuple(next_state)

            # Calculate reward
            reward = step_penalty
            if next_state in obstacles: reward = obstacle_penalty
            elif next_state == item_location: reward = item_reward
            elif next_state == goal_location: reward = goal_reward

            # Bellman expectation equation update
            v = value_function[state]
            new_value = reward + gamma * value_function[next_state]
            new_value_function[state] = new_value
            
            delta = max(delta, abs(v - new_value))

    value_function = new_value_function
    # Check for convergence
    if delta < theta:
        break

print("\nConverged Value Function for the Policy:")
print(np.round(value_function, 2))
