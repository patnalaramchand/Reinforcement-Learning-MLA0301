import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# 1. Environment Setup
class IntersectionEnv:
    def __init__(self):
        self.light_cycle = 20 # Light changes every 20 steps
        self.reset()
        
    def reset(self):
        self.car_pos = 0 # 0: waiting, 1: in intersection
        self.light_color = 0 # 0: red, 1: green
        self.time_in_cycle = 0
        return self._get_state()

    def _get_state(self):
        return np.array([self.car_pos, self.light_color, self.time_in_cycle / self.light_cycle])

    def step(self, action):
        # 0: stop, 1: go
        reward = -1 # -1 for each second of waiting
        
        # Update light
        self.time_in_cycle = (self.time_in_cycle + 1) % self.light_cycle
        if self.time_in_cycle == 0:
            self.light_color = 1 - self.light_color # Flip color
            
        collision = False
        if action == 1: # 'go'
            if self.light_color == 0: # Go on red
                reward = -10
                collision = True
            self.car_pos = 1 # Enter intersection
        
        done = collision or (self.car_pos == 1 and self.light_color == 1)
        return self._get_state(), reward, done

# 2. Actor-Critic Network
class ActorCritic(nn.Module):
    def __init__(self, state_size, action_size):
        super(ActorCritic, self).__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU()
        )
        self.actor = nn.Linear(128, action_size) # Policy head
        self.critic = nn.Linear(128, 1) # Value head

    def forward(self, x):
        x = self.shared_layer(x)
        action_probs = torch.softmax(self.actor(x), dim=-1)
        state_value = self.critic(x)
        return action_probs, state_value

# 3. A2C Algorithm
env = IntersectionEnv()
state_size = len(env.reset())
action_size = 2
model = ActorCritic(state_size, action_size)
optimizer = optim.Adam(model.parameters(), lr=5e-4)
gamma = 0.99
episodes = 1000

for episode in range(episodes):
    state = env.reset()
    done = False
    total_reward = 0
    
    while not done:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        action_probs, state_value = model(state_tensor)
        
        dist = Categorical(action_probs)
        action = dist.sample()
        
        next_state, reward, done = env.step(action.item())
        total_reward += reward
        
        # Calculate advantage
        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)
        _, next_state_value = model(next_state_tensor)
        advantage = reward + (gamma * next_state_value * (1-done)) - state_value
        
        # Calculate losses
        critic_loss = advantage.pow(2)
        actor_loss = -dist.log_prob(action) * advantage.detach() # Detach to not backprop through advantage
        
        loss = actor_loss + critic_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        state = next_state
    
    if episode % 100 == 0:
        print(f"Episode {episode}, Total Reward: {total_reward:.2f}")