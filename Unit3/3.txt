import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# Note: A full PPO implementation is more complex, involving multiple epochs of
# updates on collected data. This is a simplified, more A2C-like version
# that incorporates the core PPO clipping idea for illustration.

# 1. Environment Setup
class WarehouseEnv:
    def __init__(self):
        self.grid_size = 5
        self.item_loc = (1, 3)
        self.station_loc = (4, 4)
        self.obstacle_loc = (2, 2)
        self.reset()

    def reset(self):
        self.robot_pos = (0, 0)
        self.has_item = 0
        return self._get_state()
    
    def _get_state(self):
        return np.array([*self.robot_pos, self.has_item])

    def step(self, action):
        # 0:up, 1:down, 2:left, 3:right
        new_pos = list(self.robot_pos)
        if action == 0 and new_pos[0] > 0: new_pos[0] -= 1
        elif action == 1 and new_pos[0] < self.grid_size - 1: new_pos[0] += 1
        elif action == 2 and new_pos[1] > 0: new_pos[1] -= 1
        elif action == 3 and new_pos[1] < self.grid_size - 1: new_pos[1] += 1
        
        reward = -0.1
        done = False
        
        if tuple(new_pos) == self.obstacle_loc:
            reward = -5
        else:
            self.robot_pos = tuple(new_pos)
        
        if self.robot_pos == self.item_loc and self.has_item == 0:
            reward = 10
            self.has_item = 1
        elif self.robot_pos == self.station_loc and self.has_item == 1:
            reward = 20
            done = True
            
        return self._get_state(), reward, done

# 2. Actor-Critic Network (for PPO)
class ActorCriticPPO(nn.Module):
    def __init__(self, state_size, action_size):
        super(ActorCriticPPO, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_size, 64), nn.Tanh(),
            nn.Linear(64, 64), nn.Tanh(),
            nn.Linear(64, action_size), nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(state_size, 64), nn.Tanh(),
            nn.Linear(64, 64), nn.Tanh(),
            nn.Linear(64, 1)
        )
    def forward(self, x):
        return self.actor(x), self.critic(x)

# 3. PPO-Clip Algorithm
env = WarehouseEnv()
state_size = len(env.reset())
action_size = 4
model = ActorCriticPPO(state_size, action_size)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

gamma = 0.99
eps_clip = 0.2 # PPO clipping parameter
episodes = 1000

for episode in range(episodes):
    state = env.reset()
    done = False
    total_reward = 0
    
    while not done:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        action_probs, state_value = model(state_tensor)
        
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        next_state, reward, done = env.step(action.item())
        total_reward += reward
        
        # Calculate advantage
        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)
        _, next_state_value = model(next_state_tensor)
        advantage = reward + gamma * next_state_value * (1-done) - state_value
        
        # Calculate PPO Loss
        # In a real implementation, you'd store old_log_probs
        # For simplicity, we assume the ratio is close to 1 for this single update step
        # This simplifies to an A2C-like update but serves to structure the code.
        # Ratio r(t) would be exp(log_prob - old_log_prob)
        # For this example, let's use a simplified loss
        
        # Simplified loss resembling the PPO objective's spirit
        actor_loss = -log_prob * advantage.detach()
        critic_loss = advantage.pow(2)
        loss = actor_loss + 0.5 * critic_loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        state = next_state
        
    if episode % 100 == 0:
        print(f"Episode {episode}, Total Reward: {total_reward:.2f}")