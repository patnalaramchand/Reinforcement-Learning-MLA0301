import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# 1. Environment Setup
class CleaningRobotEnv:
    def __init__(self):
        self.grid_size = 5
        self.start_pos = (0, 0)
        self.obstacle_locs = [(1, 1), (2, 2), (3, 3), (4, 4)]
        self.reset()

    def reset(self):
        self.robot_pos = self.start_pos
        self.dirt_locs = [(0, 4), (1, 3), (2, 0), (4, 2)]
        return self._get_state()

    def _get_state(self):
        # State: robot_pos + binary flags for each dirt location
        state = list(self.robot_pos)
        dirt_flags = [1 if d in self.dirt_locs else 0 for d in [(0, 4), (1, 3), (2, 0), (4, 2)]]
        state.extend(dirt_flags)
        return np.array(state)

    def step(self, action):
        # 0:up, 1:down, 2:left, 3:right
        new_pos = list(self.robot_pos)
        if action == 0 and new_pos[0] > 0: new_pos[0] -= 1
        elif action == 1 and new_pos[0] < self.grid_size - 1: new_pos[0] += 1
        elif action == 2 and new_pos[1] > 0: new_pos[1] -= 1
        elif action == 3 and new_pos[1] < self.grid_size - 1: new_pos[1] += 1
        self.robot_pos = tuple(new_pos)

        reward = -0.1 # Small penalty for each step to encourage efficiency
        if self.robot_pos in self.obstacle_locs:
            reward = -1
        elif self.robot_pos in self.dirt_locs:
            reward = 1
            self.dirt_locs.remove(self.robot_pos)

        done = not self.dirt_locs
        return self._get_state(), reward, done

# 2. Policy Network (The "Agent")
class Policy(nn.Module):
    def __init__(self, state_size, action_size):
        super(Policy, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        return self.net(x)

# 3. REINFORCE Algorithm
env = CleaningRobotEnv()
state_size = len(env.reset())
action_size = 4
policy = Policy(state_size, action_size)
optimizer = optim.Adam(policy.parameters(), lr=1e-3)
gamma = 0.99
episodes = 2000

for episode in range(episodes):
    state = env.reset()
    log_probs = []
    rewards = []
    done = False
    
    while not done:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        action_probs = policy(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        
        log_probs.append(dist.log_prob(action))
        state, reward, done = env.step(action.item())
        rewards.append(reward)

    # Calculate discounted returns
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Normalize returns

    # Policy update
    policy_loss = []
    for log_prob, G in zip(log_probs, returns):
        policy_loss.append(-log_prob * G)
    
    optimizer.zero_grad()
    policy_loss = torch.cat(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()
    
    if episode % 100 == 0:
        print(f"Episode {episode}, Total Reward: {sum(rewards):.2f}")

print("\nTraining finished!")