import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# 1. Tic-Tac-Toe Environment
class TicTacToeEnv:
    def __init__(self):
        self.board = np.zeros(9) # 1 for agent, -1 for opponent
        self.done = False
        
    def reset(self):
        self.board = np.zeros(9)
        self.done = False
        return self.board

    def get_valid_actions(self):
        return [i for i, x in enumerate(self.board) if x == 0]

    def check_win(self, player):
        wins = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]]
        for w in wins:
            if all(self.board[i] == player for i in w):
                return True
        return False

    def step(self, action):
        if self.board[action] != 0: # Invalid move
            return self.board, -10, True # Heavy penalty
        
        self.board[action] = 1 # Agent's move
        if self.check_win(1):
            return self.board, 1, True
        if not self.get_valid_actions():
            return self.board, 0, True # Draw

        # Opponent's move (random)
        opp_action = np.random.choice(self.get_valid_actions())
        self.board[opp_action] = -1
        if self.check_win(-1):
            return self.board, -1, True
        if not self.get_valid_actions():
            return self.board, 0, True
            
        return self.board, 0, False

# 2. Policy Network
class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(9, 128), nn.ReLU(),
            nn.Linear(128, 9)
        )
    def forward(self, x, valid_actions):
        logits = self.net(x)
        # Mask invalid actions
        mask = torch.full(logits.shape, -np.inf)
        mask[valid_actions] = 0
        return torch.softmax(logits + mask, dim=-1)

# 3. VPG/REINFORCE Algorithm
env = TicTacToeEnv()
policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
gamma = 0.99
episodes = 5000

for episode in range(episodes):
    state = env.reset()
    log_probs = []
    rewards = []
    done = False
    
    while not done:
        state_tensor = torch.from_numpy(state).float()
        valid_actions = env.get_valid_actions()
        action_probs = policy(state_tensor, valid_actions)
        
        dist = Categorical(action_probs)
        action = dist.sample()
        
        log_probs.append(dist.log_prob(action))
        state, reward, done = env.step(action.item())
        rewards.append(reward)

    # We only care about the final reward, but let's discount for form's sake
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    policy_loss = [-log_prob * G for log_prob, G in zip(log_probs, returns)]
    
    optimizer.zero_grad()
    policy_loss = torch.stack(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()
    
    if episode % 500 == 0:
        print(f"Episode {episode}, Final Reward: {rewards[-1]}")