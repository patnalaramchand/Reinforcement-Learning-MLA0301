import numpy as np
import matplotlib.pyplot as plt

# --- 1. Environment and Robotic Arm Model ---
# Define the physical properties of the arm
L1 = 1.0  # Length of link 1
L2 = 1.0  # Length of link 2
target_pos = np.array([1.5, 1.0]) # Target (x, y) position

# The model: Forward Kinematics
# This function calculates the end-effector's position from joint angles.
def forward_kinematics(theta1, theta2):
    """Calculates the (x, y) position of the arm's end."""
    x = L1 * np.cos(theta1) + L2 * np.cos(theta1 + theta2)
    y = L1 * np.sin(theta1) + L2 * np.sin(theta1 + theta2)
    return np.array([x, y])

# --- 2. Analytic Gradient Computation ---
def compute_gradient(theta1, theta2, current_pos, target_pos):
    """
    Computes the analytic gradient of the loss (squared distance)
    with respect to the joint angles (theta1, theta2).
    This uses the chain rule: d(loss)/d(theta) = d(loss)/d(pos) * d(pos)/d(theta)
    """
    # Gradient of loss w.r.t. position: d(loss)/d(pos)
    error = current_pos - target_pos
    grad_loss_pos = 2 * error

    # Jacobian Matrix: d(pos)/d(theta)
    # This is the derivative of the forward kinematics function.
    dx_dtheta1 = -L1 * np.sin(theta1) - L2 * np.sin(theta1 + theta2)
    dx_dtheta2 = -L2 * np.sin(theta1 + theta2)
    dy_dtheta1 = L1 * np.cos(theta1) + L2 * np.cos(theta1 + theta2)
    dy_dtheta2 = L2 * np.cos(theta1 + theta2)
    
    jacobian = np.array([[dx_dtheta1, dx_dtheta2],
                         [dy_dtheta1, dy_dtheta2]])

    # Final gradient: d(loss)/d(theta)
    gradient = jacobian.T @ grad_loss_pos
    return gradient

# --- 3. Optimization Process ---
# Initial state of the robotic arm (joint angles)
thetas = np.array([np.pi / 4, np.pi / 4]) # Start at 45 degrees for both joints
learning_rate = 0.01
num_iterations = 100
path_history = []

print("Optimizing robotic arm policy using analytic gradients...")
for i in range(num_iterations):
    # Get current position using the model
    current_pos = forward_kinematics(thetas[0], thetas[1])
    path_history.append(current_pos)
    
    # Calculate the error (loss)
    loss = np.sum((current_pos - target_pos)**2)
    if i % 10 == 0:
        print(f"Iteration {i}, Position: {current_pos}, Error: {loss:.4f}")

    if loss < 1e-4:
        print(f"\nTarget reached at iteration {i}!")
        break
        
    # Compute the gradient analytically
    grad = compute_gradient(thetas[0], thetas[1], current_pos, target_pos)
    
    # Update the policy (the joint angles) using gradient descent
    thetas -= learning_rate * grad

print(f"\nFinal Position: {forward_kinematics(thetas[0], thetas[1])}")
print(f"Target Position: {target_pos}")

# --- 4. Visualization ---
path_history = np.array(path_history)
plt.figure(figsize=(8, 8))
plt.plot(path_history[:, 0], path_history[:, 1], 'r--', label='End-Effector Path')
plt.plot(target_pos[0], target_pos[1], 'g*', markersize=15, label='Target')
plt.plot(path_history[0, 0], path_history[0, 1], 'bo', markersize=10, label='Start')
plt.title("Robotic Arm Optimization")
plt.xlabel("X Position")
plt.ylabel("Y Position")
plt.legend()
plt.grid(True)
plt.axis('equal')
plt.show()